{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "from trainer import Trainer, TrainerArgs\n",
    "\n",
    "# from TTS.bin.compute_embeddings import compute_embeddings\n",
    "from compute_embeddings import compute_embeddings # use custom formatter without forking the lib\n",
    "from TTS.bin.resample import resample_files\n",
    "from TTS.config.shared_configs import BaseDatasetConfig\n",
    "from TTS.tts.configs.vits_config import VitsConfig\n",
    "from TTS.tts.datasets import load_tts_samples\n",
    "from TTS.tts.models.vits import CharactersConfig, Vits, VitsArgs, VitsAudioConfig\n",
    "# from TTS.utils.downloaders import download_vctk\n",
    "# from TTS.config import load_config\n",
    "# from TTS.config.shared_configs import BaseDatasetConfig\n",
    "# from TTS.tts.datasets import load_tts_samples\n",
    "# from TTS.tts.utils.managers import save_file\n",
    "# from TTS.tts.utils.speakers import SpeakerManager\n",
    "from TTS.tts.datasets.formatters import vctk\n",
    "from functools import partial\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "torch.set_num_threads(24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Current path\n",
    "CURRENT_PATH = os.getcwd()\n",
    "\n",
    "# Name of the run for the Trainer\n",
    "RUN_NAME = \"KhongKhunTTS-TH-VCTK\"\n",
    "\n",
    "# Path where you want to save the models outputs (configs, checkpoints and tensorboard logs)\n",
    "OUT_PATH = os.getcwd()\n",
    "\n",
    "# If you want to do transfer learning and speedup your training you can set here the path to the model\n",
    "RESTORE_PATH = None\n",
    "\n",
    "# This paramter is useful to debug, it skips the training epochs and just do the evaluation  and produce the test sentences\n",
    "SKIP_TRAIN_EPOCH = False\n",
    "\n",
    "# Set here the batch size to be used in training and evaluation\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Training Sampling rate and the target sampling rate for resampling the downloaded dataset (Note: If you change this you might need to redownload the dataset !!)\n",
    "# Note: If you add new datasets, please make sure that the dataset sampling rate and this parameter are matching, otherwise resample your audios\n",
    "SAMPLE_RATE = 32000\n",
    "\n",
    "# Max audio length in seconds to be used in training (every audio bigger than it will be ignored)\n",
    "MAX_AUDIO_LEN_IN_SECONDS = 10\n",
    "\n",
    "### Download VCTK dataset\n",
    "VCTK_DOWNLOAD_PATH = os.path.join(CURRENT_PATH, \"commonvoice-to-vctk\")\n",
    "# Define the number of threads used during the audio resampling\n",
    "NUM_RESAMPLE_THREADS = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if VCTK dataset is not already downloaded, if not download it\n",
    "# if not os.path.exists(VCTK_DOWNLOAD_PATH):\n",
    "#     print(\">>> Downloading VCTK dataset:\")\n",
    "#     download_vctk(VCTK_DOWNLOAD_PATH, True)\n",
    "    # resample_files(VCTK_DOWNLOAD_PATH, SAMPLE_RATE, file_ext=\"flac\", n_jobs=NUM_RESAMPLE_THREADS)\n",
    "\n",
    "# init configs\n",
    "vctk_config = BaseDatasetConfig(\n",
    "    formatter=\"vctk_32k\",\n",
    "    dataset_name=\"vctk\",\n",
    "    meta_file_train=\"\",\n",
    "    meta_file_val=\"\",\n",
    "    path=VCTK_DOWNLOAD_PATH,\n",
    "    language=\"th\",\n",
    "    ignored_speakers=[\n",
    "        \"cv017\", # Female Teenager\n",
    "        \"cv048\", # Female Teenager\n",
    "        \"cv039\", # Female Adult\n",
    "        \"cv052\", # Female Adult\n",
    "        \"cv069\", # Male Teenager\n",
    "        \"cv054\", # Male Teenager\n",
    "        \"cv049\", # Male Adult\n",
    "        \"cv026\", # Male Adult\n",
    "    ], # For testing set\n",
    ")\n",
    "\n",
    "# Add here all datasets configs, in our case we just want to train with the VCTK dataset then we need to add just VCTK. Note: If you want to add new datasets, just add them here and it will automatically compute the speaker embeddings (d-vectors) for this new dataset :)\n",
    "DATASETS_CONFIG_LIST = [vctk_config]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract speaker embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tqdm import tqdm\n",
    "\n",
    "# from TTS.config import load_config\n",
    "# from TTS.config.shared_configs import BaseDatasetConfig\n",
    "# from TTS.tts.datasets import load_tts_samples\n",
    "# from TTS.tts.utils.managers import save_file\n",
    "# from TTS.tts.utils.speakers import SpeakerManager\n",
    "# from TTS.tts.datasets.formatters import vctk\n",
    "# from functools import partial\n",
    "\n",
    "# def compute_embeddings(\n",
    "#     model_path,\n",
    "#     config_path,\n",
    "#     output_path,\n",
    "#     formatter_name=None,\n",
    "#     dataset_name=None,\n",
    "#     dataset_path=None,\n",
    "#     meta_file_train=None,\n",
    "#     meta_file_val=None,\n",
    "# ):\n",
    "#     use_cuda = torch.cuda.is_available()\n",
    "\n",
    "#     c_dataset = BaseDatasetConfig()\n",
    "#     c_dataset.formatter = formatter_name\n",
    "#     c_dataset.dataset_name = dataset_name\n",
    "#     c_dataset.path = dataset_path\n",
    "#     if meta_file_train is not None:\n",
    "#         c_dataset.meta_file_train = meta_file_train\n",
    "#     if meta_file_val is not None:\n",
    "#         c_dataset.meta_file_val = meta_file_val\n",
    "#     meta_data_train, meta_data_eval = load_tts_samples(c_dataset, eval_split=True, formatter=custom_vctk)\n",
    "    \n",
    "#     samples = meta_data_train + meta_data_eval\n",
    "\n",
    "#     encoder_manager = SpeakerManager(\n",
    "#         encoder_model_path=model_path,\n",
    "#         encoder_config_path=config_path,\n",
    "#         d_vectors_file_path=None,\n",
    "#         use_cuda=use_cuda,\n",
    "#     )\n",
    "\n",
    "#     class_name_key = encoder_manager.encoder_config.class_name_key\n",
    "\n",
    "#     # compute speaker embeddings\n",
    "#     speaker_mapping = {}\n",
    "\n",
    "#     for fields in tqdm(samples):\n",
    "#         class_name = fields[class_name_key]\n",
    "#         audio_file = fields[\"audio_file\"]\n",
    "#         embedding_key = fields[\"audio_unique_name\"]\n",
    "\n",
    "#         # Only update the speaker name when the embedding is already in the old file.\n",
    "#         if embedding_key in speaker_mapping:\n",
    "#             speaker_mapping[embedding_key][\"name\"] = class_name\n",
    "#             continue\n",
    "\n",
    "#         embedd = encoder_manager.compute_embedding_from_clip(audio_file)\n",
    "\n",
    "#         # create speaker_mapping if target dataset is defined\n",
    "#         speaker_mapping[embedding_key] = {}\n",
    "#         speaker_mapping[embedding_key][\"name\"] = class_name\n",
    "#         speaker_mapping[embedding_key][\"embedding\"] = embedd\n",
    "\n",
    "#     if speaker_mapping:\n",
    "#         # save speaker_mapping if target dataset is defined\n",
    "#         if os.path.isdir(output_path):\n",
    "#             mapping_file_path = os.path.join(output_path, \"speakers.pth\")\n",
    "#         else:\n",
    "#             mapping_file_path = output_path\n",
    "\n",
    "#         if os.path.dirname(mapping_file_path) != \"\":\n",
    "#             os.makedirs(os.path.dirname(mapping_file_path), exist_ok=True)\n",
    "\n",
    "#         save_file(speaker_mapping, mapping_file_path)\n",
    "#         print(\"Speaker embeddings saved at:\", mapping_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPEAKER_ENCODER_CHECKPOINT_PATH = (\n",
    "    \"https://github.com/coqui-ai/TTS/releases/download/speaker_encoder_model/model_se.pth.tar\"\n",
    ")\n",
    "SPEAKER_ENCODER_CONFIG_PATH = \"https://github.com/coqui-ai/TTS/releases/download/speaker_encoder_model/config_se.json\"\n",
    "\n",
    "D_VECTOR_FILES = []  # List of speaker embeddings/d-vectors to be used during the training\n",
    "\n",
    "vctk_32k = partial(\n",
    "    vctk,\n",
    "    wavs_path=\"wav32_silence_trimmed\",\n",
    ")\n",
    "\n",
    "# Iterates all the dataset configs checking if the speakers embeddings are already computated, if not compute it\n",
    "for dataset_conf in DATASETS_CONFIG_LIST:\n",
    "    # Check if the embeddings weren't already computed, if not compute it\n",
    "    embeddings_file = os.path.join(vctk_config.path, \"speakers.pth\")\n",
    "    if not os.path.isfile(embeddings_file):\n",
    "        print(f\">>> Computing the speaker embeddings for the {vctk_config.dataset_name} dataset\")\n",
    "        compute_embeddings(\n",
    "            SPEAKER_ENCODER_CHECKPOINT_PATH,\n",
    "            SPEAKER_ENCODER_CONFIG_PATH,\n",
    "            embeddings_file,\n",
    "            formatter_name=vctk_config.formatter,\n",
    "            formatter=vctk_32k if vctk_config.formatter == \"vctk_32k\" else None,\n",
    "            dataset_name=vctk_config.dataset_name,\n",
    "            dataset_path=vctk_config.path,\n",
    "            meta_file_train=vctk_config.meta_file_train,\n",
    "            meta_file_val=vctk_config.meta_file_val,\n",
    "        )\n",
    "\n",
    "        # meta_data_train, meta_data_eval = load_tts_samples(vctk_config, eval_split=True, formatter=custom_vctk)\n",
    "\n",
    "        # samples = meta_data_train + meta_data_eval\n",
    "\n",
    "        # encoder_manager = SpeakerManager(\n",
    "        #     encoder_model_path=SPEAKER_ENCODER_CHECKPOINT_PATH,\n",
    "        #     encoder_config_path=SPEAKER_ENCODER_CONFIG_PATH,\n",
    "        #     d_vectors_file_path=None,\n",
    "        #     use_cuda=torch.cuda.is_available(),\n",
    "        # )\n",
    "\n",
    "        # class_name_key = encoder_manager.encoder_config.class_name_key\n",
    "\n",
    "        # # compute speaker embeddings\n",
    "        # speaker_mapping = {}\n",
    "\n",
    "        # for fields in tqdm(samples):\n",
    "        #     class_name = fields[class_name_key]\n",
    "        #     audio_file = fields[\"audio_file\"]\n",
    "        #     embedding_key = fields[\"audio_unique_name\"]\n",
    "\n",
    "        #     # Only update the speaker name when the embedding is already in the old file.\n",
    "        #     if embedding_key in speaker_mapping:\n",
    "        #         speaker_mapping[embedding_key][\"name\"] = class_name\n",
    "        #         continue\n",
    "\n",
    "        #     embedd = encoder_manager.compute_embedding_from_clip(audio_file)\n",
    "\n",
    "        #     # create speaker_mapping if target dataset is defined\n",
    "        #     speaker_mapping[embedding_key] = {}\n",
    "        #     speaker_mapping[embedding_key][\"name\"] = class_name\n",
    "        #     speaker_mapping[embedding_key][\"embedding\"] = embedd\n",
    "        \n",
    "        # if speaker_mapping:\n",
    "        #     # save speaker_mapping if target dataset is defined\n",
    "        #     if os.path.isdir(embeddings_file):\n",
    "        #         mapping_file_path = os.path.join(embeddings_file, \"speakers.pth\")\n",
    "        #     else:\n",
    "        #         mapping_file_path = embeddings_file\n",
    "\n",
    "        #     if os.path.dirname(mapping_file_path) != \"\":\n",
    "        #         os.makedirs(os.path.dirname(mapping_file_path), exist_ok=True)\n",
    "\n",
    "        #     save_file(speaker_mapping, mapping_file_path)\n",
    "        #     print(\"Speaker embeddings saved at:\", mapping_file_path)\n",
    "\n",
    "    D_VECTOR_FILES.append(embeddings_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audio config used in training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_config = VitsAudioConfig(\n",
    "    sample_rate=SAMPLE_RATE,\n",
    "    hop_length=256,\n",
    "    win_length=1024,\n",
    "    fft_size=1024,\n",
    "    mel_fmin=0.0,\n",
    "    mel_fmax=None,\n",
    "    num_mels=80,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init VITSArgs setting the arguments that are needed for the KhongKhunTTS model\n",
    "model_args = VitsArgs(\n",
    "    d_vector_file=D_VECTOR_FILES,\n",
    "    use_d_vector_file=True,\n",
    "    d_vector_dim=512,\n",
    "    num_layers_text_encoder=10,\n",
    "    speaker_encoder_model_path=SPEAKER_ENCODER_CHECKPOINT_PATH,\n",
    "    speaker_encoder_config_path=SPEAKER_ENCODER_CONFIG_PATH,\n",
    "    resblock_type_decoder=\"2\",  # In the YourTTS paper, trained using ResNet blocks type 2, if you like you can use the ResNet blocks type 1 like the VITS model\n",
    "    # Useful parameters to enable the Speaker Consistency Loss (SCL) described in the paper\n",
    "    # use_speaker_encoder_as_loss=True,\n",
    "    # Useful parameters to enable multilingual training\n",
    "    # use_language_embedding=True,\n",
    "    # embedded_language_dim=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General training config, here you can change the batch size and others useful parameters\n",
    "config = VitsConfig(\n",
    "    output_path=OUT_PATH,\n",
    "    model_args=model_args,\n",
    "    run_name=RUN_NAME,\n",
    "    project_name=\"KhongKhunTTS\",\n",
    "    run_description=\"\"\"\n",
    "            - KhongKhunTTS trained using CommonVoiceTH (VCTK structure)\n",
    "        \"\"\",\n",
    "    dashboard_logger=\"tensorboard\",\n",
    "    logger_uri=None,\n",
    "    audio=audio_config,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    batch_group_size=48,\n",
    "    eval_batch_size=BATCH_SIZE,\n",
    "    num_loader_workers=8,\n",
    "    eval_split_max_size=256,\n",
    "    print_step=50,\n",
    "    plot_step=100,\n",
    "    log_model_step=1000,\n",
    "    save_step=5000,\n",
    "    save_n_checkpoints=2,\n",
    "    save_checkpoints=True,\n",
    "    target_loss=\"loss_1\",\n",
    "    print_eval=False,\n",
    "    use_phonemes=False,\n",
    "    phonemizer=\"espeak\",\n",
    "    phoneme_language=\"en\",\n",
    "    compute_input_seq_cache=True,\n",
    "    add_blank=True,\n",
    "    text_cleaner=\"multilingual_cleaners\",\n",
    "    characters=CharactersConfig(\n",
    "        characters_class=\"TTS.tts.models.vits.VitsCharacters\",\n",
    "        pad=\"_\",\n",
    "        eos=\"&\",\n",
    "        bos=\"*\",\n",
    "        blank=None,\n",
    "        characters=\"ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\\u00af\\u00b7\\u00df\\u00e0\\u00e1\\u00e2\\u00e3\\u00e4\\u00e6\\u00e7\\u00e8\\u00e9\\u00ea\\u00eb\\u00ec\\u00ed\\u00ee\\u00ef\\u00f1\\u00f2\\u00f3\\u00f4\\u00f5\\u00f6\\u00f9\\u00fa\\u00fb\\u00fc\\u00ff\\u0101\\u0105\\u0107\\u0113\\u0119\\u011b\\u012b\\u0131\\u0142\\u0144\\u014d\\u0151\\u0153\\u015b\\u016b\\u0171\\u017a\\u017c\\u01ce\\u01d0\\u01d2\\u01d4\\u0430\\u0431\\u0432\\u0433\\u0434\\u0435\\u0436\\u0437\\u0438\\u0439\\u043a\\u043b\\u043c\\u043d\\u043e\\u043f\\u0440\\u0441\\u0442\\u0443\\u0444\\u0445\\u0446\\u0447\\u0448\\u0449\\u044a\\u044b\\u044c\\u044d\\u044e\\u044f\\u0451\\u0454\\u0456\\u0457\\u0491\\u2013!\\\"'(),-.:;?|~ \\u0e01\\u0e02\\u0e04\\u0e06\\u0e07\\u0e08\\u0e09\\u0e0a\\u0e0b\\u0e0c\\u0e0d\\u0e0e\\u0e0f\\u0e10\\u0e11\\u0e12\\u0e13\\u0e14\\u0e15\\u0e16\\u0e17\\u0e18\\u0e19\\u0e1a\\u0e1b\\u0e1c\\u0e1d\\u0e1e\\u0e1f\\u0e20\\u0e21\\u0e22\\u0e23\\u0e24\\u0e25\\u0e27\\u0e28\\u0e29\\u0e2a\\u0e2b\\u0e2c\\u0e2d\\u0e2e\\u0e2f\\u0e30\\u0e31\\u0e32\\u0e33\\u0e34\\u0e35\\u0e36\\u0e37\\u0e38\\u0e39\\u0e40\\u0e41\\u0e42\\u0e43\\u0e44\\u0e45\\u0e46\\u0e47\\u0e48\\u0e49\\u0e4a\\u0e4b\\u0e4c\\u0e4d\\u2014\\u2018\\u2019\\u201c\\u201d\",\n",
    "        punctuations=\"!\\\"'(),-.:;?|~ \",\n",
    "        phonemes=\"\",\n",
    "        is_unique=True,\n",
    "        is_sorted=True,\n",
    "    ),\n",
    "    phoneme_cache_path=None,\n",
    "    precompute_num_workers=12,\n",
    "    start_by_longest=True,\n",
    "    datasets=DATASETS_CONFIG_LIST,\n",
    "    cudnn_benchmark=False,\n",
    "    max_audio_len=SAMPLE_RATE * MAX_AUDIO_LEN_IN_SECONDS,\n",
    "    mixed_precision=False,\n",
    "    test_sentences=[\n",
    "        [\n",
    "            \"‡∏â‡∏±‡∏ô‡πÉ‡∏ä‡πâ‡πÄ‡∏ß‡∏•‡∏≤‡∏ô‡∏≤‡∏ô‡∏°‡∏≤‡∏Å‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏û‡∏±‡∏í‡∏ô‡∏≤‡πÄ‡∏™‡∏µ‡∏¢‡∏á ‡πÅ‡∏•‡∏∞‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡∏â‡∏±‡∏ô‡∏°‡∏µ‡∏°‡∏±‡∏ô‡πÅ‡∏•‡πâ‡∏ß ‡∏â‡∏±‡∏ô‡∏à‡∏∞‡πÑ‡∏°‡πà‡πÄ‡∏á‡∏µ‡∏¢‡∏ö‡∏≠‡∏µ‡∏Å‡∏ï‡πà‡∏≠‡πÑ‡∏õ\",\n",
    "            \"VCTK_cv005\",\n",
    "            None,\n",
    "            \"th\",\n",
    "        ],\n",
    "        [\n",
    "            \"‡∏¢‡∏±‡∏Å‡∏©‡πå‡πÉ‡∏´‡∏ç‡πà‡πÑ‡∏•‡πà‡∏¢‡∏±‡∏Å‡∏©‡πå‡πÄ‡∏•‡πá‡∏Å ‡∏¢‡∏±‡∏Å‡∏©‡πå‡πÄ‡∏•‡πá‡∏Å‡πÑ‡∏•‡πà‡∏¢‡∏±‡∏Å‡∏©‡πå‡πÉ‡∏´‡∏ç‡πà\",\n",
    "            \"VCTK_cv068\",\n",
    "            None,\n",
    "            \"th\",\n",
    "        ],\n",
    "        [\n",
    "            \"‡∏¢‡∏≤‡∏¢‡∏Å‡∏¥‡∏ô‡∏•‡∏≥‡πÑ‡∏¢‡∏ô‡πâ‡∏≥‡∏•‡∏≤‡∏¢‡∏¢‡∏≤‡∏¢‡πÑ‡∏´‡∏•‡∏¢‡πâ‡∏≠‡∏¢\",\n",
    "            \"VCTK_cv057\",\n",
    "            None,\n",
    "            \"th\",\n",
    "        ],\n",
    "        [\n",
    "            \"‡∏ä‡∏≤‡∏°‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ß‡∏Ñ‡∏ß‡πà‡∏≥‡πÄ‡∏ä‡πâ‡∏≤ ‡∏ä‡∏≤‡∏°‡∏Ç‡∏≤‡∏°‡∏Ñ‡∏ß‡πà‡∏≥‡∏Ñ‡πà‡∏≥\",\n",
    "            \"VCTK_cv103\",\n",
    "            None,\n",
    "            \"th\",\n",
    "        ],\n",
    "        [\n",
    "            \"‡∏´‡∏°‡∏≠‡∏ô‡∏•‡∏≠‡∏¢‡∏ô‡πâ‡∏≥‡∏°‡∏≤‡∏ß‡πà‡∏≤‡∏¢‡∏ô‡πâ‡∏≥‡πÑ‡∏õ‡∏ñ‡∏≠‡∏¢‡∏´‡∏°‡∏≠‡∏ô\",\n",
    "            \"VCTK_cv133\",\n",
    "            None,\n",
    "            \"th\",\n",
    "        ],\n",
    "        [\n",
    "            \"‡πÄ‡∏ä‡πâ‡∏≤‡∏ü‡∏≤‡∏î‡∏ú‡∏±‡∏î‡∏ü‡∏±‡∏Å ‡πÄ‡∏¢‡πá‡∏ô‡∏ü‡∏≤‡∏î‡∏ü‡∏±‡∏Å‡∏ú‡∏±‡∏î\",\n",
    "            \"VCTK_cv128\",\n",
    "            None,\n",
    "            \"th\",\n",
    "        ],\n",
    "    ],\n",
    "    # # Enable the weighted sampler\n",
    "    # use_weighted_sampler=True,\n",
    "    # # Ensures that all speakers are seen in the training batch equally no matter how many samples each speaker has\n",
    "    # weighted_sampler_attrs={\"speaker_name\": 1.0},\n",
    "    # weighted_sampler_multipliers={\"Makeitnotblack\": None},\n",
    "\n",
    "    # It defines the Speaker Consistency Loss (SCL) Œ± to 9 like the paper\n",
    "    speaker_encoder_loss_alpha=9.0,\n",
    ")\n",
    "\n",
    "# config.weighted_sampler_multipliers = {}\n",
    "\n",
    "# print(config.weighted_sampler_multipliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all the datasets samples and split traning and evaluation sets\n",
    "train_samples, eval_samples = load_tts_samples(\n",
    "    config.datasets,\n",
    "    eval_split=True,\n",
    "    eval_split_max_size=config.eval_split_max_size,\n",
    "    eval_split_size=config.eval_split_size,\n",
    "    formatter=vctk_32k if vctk_config.formatter == \"vctk_32k\" else None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init the model\n",
    "model = Vits.init_from_config(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init the trainer and üöÄ\n",
    "trainer = Trainer(\n",
    "    TrainerArgs(restore_path=RESTORE_PATH, skip_train_epoch=SKIP_TRAIN_EPOCH),\n",
    "    config,\n",
    "    output_path=OUT_PATH,\n",
    "    model=model,\n",
    "    train_samples=train_samples,\n",
    "    eval_samples=eval_samples,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weighted_sampler_multipliers={}\n",
    "# weighted_sampler_multipliers.get(0, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from dataclasses import dataclass, field\n",
    "# weighted_sampler_multipliers: dict = field(default_factory=lambda: {})\n",
    "\n",
    "# print(weighted_sampler_multipliers)\n",
    "\n",
    "# weighted_sampler_multipliers.get(0, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @dataclass\n",
    "# class MyClass:\n",
    "#     weighted_sampler_multipliers: dict = field(default_factory=lambda: {})\n",
    "\n",
    "# obj = MyClass()\n",
    "# print(obj.weighted_sampler_multipliers)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "khongkhuntts-CfIBPH6q-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
