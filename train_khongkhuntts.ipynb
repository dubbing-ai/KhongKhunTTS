{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "from trainer import Trainer, TrainerArgs\n",
    "\n",
    "from TTS.bin.compute_embeddings import compute_embeddings\n",
    "from TTS.bin.resample import resample_files\n",
    "from TTS.config.shared_configs import BaseDatasetConfig\n",
    "from TTS.tts.configs.vits_config import VitsConfig\n",
    "from TTS.tts.datasets import load_tts_samples\n",
    "from TTS.tts.models.vits import CharactersConfig, Vits, VitsArgs, VitsAudioConfig\n",
    "from TTS.utils.downloaders import download_vctk\n",
    "\n",
    "torch.set_num_threads(24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Current path\n",
    "CURRENT_PATH = os.path.dirname(os.path.abspath(__file__))\n",
    "\n",
    "# Name of the run for the Trainer\n",
    "RUN_NAME = \"KhongKhunTTS-TH-VCTK\"\n",
    "\n",
    "# Path where you want to save the models outputs (configs, checkpoints and tensorboard logs)\n",
    "OUT_PATH = os.path.dirname(os.path.abspath(__file__))\n",
    "\n",
    "# If you want to do transfer learning and speedup your training you can set here the path to the model\n",
    "RESTORE_PATH = None\n",
    "\n",
    "# This paramter is useful to debug, it skips the training epochs and just do the evaluation  and produce the test sentences\n",
    "SKIP_TRAIN_EPOCH = False\n",
    "\n",
    "# Set here the batch size to be used in training and evaluation\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Training Sampling rate and the target sampling rate for resampling the downloaded dataset (Note: If you change this you might need to redownload the dataset !!)\n",
    "# Note: If you add new datasets, please make sure that the dataset sampling rate and this parameter are matching, otherwise resample your audios\n",
    "SAMPLE_RATE = 16000\n",
    "\n",
    "# Max audio length in seconds to be used in training (every audio bigger than it will be ignored)\n",
    "MAX_AUDIO_LEN_IN_SECONDS = 10\n",
    "\n",
    "### Download VCTK dataset\n",
    "VCTK_DOWNLOAD_PATH = os.path.join(CURRENT_PATH, \"VCTK\")\n",
    "# Define the number of threads used during the audio resampling\n",
    "NUM_RESAMPLE_THREADS = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if VCTK dataset is not already downloaded, if not download it\n",
    "if not os.path.exists(VCTK_DOWNLOAD_PATH):\n",
    "    print(\">>> Downloading VCTK dataset:\")\n",
    "    download_vctk(VCTK_DOWNLOAD_PATH)\n",
    "    resample_files(VCTK_DOWNLOAD_PATH, SAMPLE_RATE, file_ext=\"flac\", n_jobs=NUM_RESAMPLE_THREADS)\n",
    "\n",
    "# init configs\n",
    "vctk_config = BaseDatasetConfig(\n",
    "    formatter=\"vctk\",\n",
    "    dataset_name=\"vctk\",\n",
    "    meta_file_train=\"\",\n",
    "    meta_file_val=\"\",\n",
    "    path=VCTK_DOWNLOAD_PATH,\n",
    "    language=\"th\",\n",
    "    ignored_speakers=[\n",
    "        \"cv017\", # Female Teenager\n",
    "        \"cv048\", # Female Teenager\n",
    "        \"cv039\", # Female Adult\n",
    "        \"cv052\", # Female Adult\n",
    "        \"cv026\", # Male Teenager\n",
    "        \"cv054\", # Male Teenager\n",
    "        \"cv049\", # Male Adult\n",
    "        \"cv026\", # Male Adult\n",
    "    ], # For testing set\n",
    ")\n",
    "\n",
    "# Add here all datasets configs, in our case we just want to train with the VCTK dataset then we need to add just VCTK. Note: If you want to add new datasets, just add them here and it will automatically compute the speaker embeddings (d-vectors) for this new dataset :)\n",
    "DATASETS_CONFIG_LIST = [vctk_config]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract speaker embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPEAKER_ENCODER_CHECKPOINT_PATH = (\n",
    "    \"https://github.com/coqui-ai/TTS/releases/download/speaker_encoder_model/model_se.pth.tar\"\n",
    ")\n",
    "SPEAKER_ENCODER_CONFIG_PATH = \"https://github.com/coqui-ai/TTS/releases/download/speaker_encoder_model/config_se.json\"\n",
    "\n",
    "D_VECTOR_FILES = []  # List of speaker embeddings/d-vectors to be used during the training\n",
    "\n",
    "# Iterates all the dataset configs checking if the speakers embeddings are already computated, if not compute it\n",
    "for dataset_conf in DATASETS_CONFIG_LIST:\n",
    "    # Check if the embeddings weren't already computed, if not compute it\n",
    "    embeddings_file = os.path.join(dataset_conf.path, \"speakers.pth\")\n",
    "    if not os.path.isfile(embeddings_file):\n",
    "        print(f\">>> Computing the speaker embeddings for the {dataset_conf.dataset_name} dataset\")\n",
    "        compute_embeddings(\n",
    "            SPEAKER_ENCODER_CHECKPOINT_PATH,\n",
    "            SPEAKER_ENCODER_CONFIG_PATH,\n",
    "            embeddings_file,\n",
    "            old_speakers_file=None,\n",
    "            config_dataset_path=None,\n",
    "            formatter_name=dataset_conf.formatter,\n",
    "            dataset_name=dataset_conf.dataset_name,\n",
    "            dataset_path=dataset_conf.path,\n",
    "            meta_file_train=dataset_conf.meta_file_train,\n",
    "            meta_file_val=dataset_conf.meta_file_val,\n",
    "            disable_cuda=False,\n",
    "            no_eval=False,\n",
    "        )\n",
    "    D_VECTOR_FILES.append(embeddings_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audio config used in training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_config = VitsAudioConfig(\n",
    "    sample_rate=SAMPLE_RATE,\n",
    "    hop_length=256,\n",
    "    win_length=1024,\n",
    "    fft_size=1024,\n",
    "    mel_fmin=0.0,\n",
    "    mel_fmax=None,\n",
    "    num_mels=80,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init VITSArgs setting the arguments that are needed for the KhongKhunTTS model\n",
    "model_args = VitsArgs(\n",
    "    d_vector_file=D_VECTOR_FILES,\n",
    "    use_d_vector_file=True,\n",
    "    d_vector_dim=512,\n",
    "    num_layers_text_encoder=10,\n",
    "    speaker_encoder_model_path=SPEAKER_ENCODER_CHECKPOINT_PATH,\n",
    "    speaker_encoder_config_path=SPEAKER_ENCODER_CONFIG_PATH,\n",
    "    resblock_type_decoder=\"2\",  # In the YourTTS paper, trained using ResNet blocks type 2, if you like you can use the ResNet blocks type 1 like the VITS model\n",
    "    # Useful parameters to enable the Speaker Consistency Loss (SCL) described in the paper\n",
    "    # use_speaker_encoder_as_loss=True,\n",
    "    # Useful parameters to enable multilingual training\n",
    "    # use_language_embedding=True,\n",
    "    # embedded_language_dim=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General training config, here you can change the batch size and others useful parameters\n",
    "config = VitsConfig(\n",
    "    output_path=OUT_PATH,\n",
    "    model_args=model_args,\n",
    "    run_name=RUN_NAME,\n",
    "    project_name=\"KhongKhunTTS\",\n",
    "    run_description=\"\"\"\n",
    "            - KhongKhunTTS trained using CommonVoiceTH (VCTK structure)\n",
    "        \"\"\",\n",
    "    dashboard_logger=\"tensorboard\",\n",
    "    logger_uri=None,\n",
    "    audio=audio_config,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    batch_group_size=48,\n",
    "    eval_batch_size=BATCH_SIZE,\n",
    "    num_loader_workers=8,\n",
    "    eval_split_max_size=256,\n",
    "    print_step=50,\n",
    "    plot_step=100,\n",
    "    log_model_step=1000,\n",
    "    save_step=5000,\n",
    "    save_n_checkpoints=2,\n",
    "    save_checkpoints=True,\n",
    "    target_loss=\"loss_1\",\n",
    "    print_eval=False,\n",
    "    use_phonemes=False,\n",
    "    phonemizer=\"espeak\",\n",
    "    phoneme_language=\"en\",\n",
    "    compute_input_seq_cache=True,\n",
    "    add_blank=True,\n",
    "    text_cleaner=\"multilingual_cleaners\",\n",
    "    characters=CharactersConfig(\n",
    "        characters_class=\"TTS.tts.models.vits.VitsCharacters\",\n",
    "        pad=\"_\",\n",
    "        eos=\"&\",\n",
    "        bos=\"*\",\n",
    "        blank=None,\n",
    "        characters=\"ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\\u00af\\u00b7\\u00df\\u00e0\\u00e1\\u00e2\\u00e3\\u00e4\\u00e6\\u00e7\\u00e8\\u00e9\\u00ea\\u00eb\\u00ec\\u00ed\\u00ee\\u00ef\\u00f1\\u00f2\\u00f3\\u00f4\\u00f5\\u00f6\\u00f9\\u00fa\\u00fb\\u00fc\\u00ff\\u0101\\u0105\\u0107\\u0113\\u0119\\u011b\\u012b\\u0131\\u0142\\u0144\\u014d\\u0151\\u0153\\u015b\\u016b\\u0171\\u017a\\u017c\\u01ce\\u01d0\\u01d2\\u01d4\\u0430\\u0431\\u0432\\u0433\\u0434\\u0435\\u0436\\u0437\\u0438\\u0439\\u043a\\u043b\\u043c\\u043d\\u043e\\u043f\\u0440\\u0441\\u0442\\u0443\\u0444\\u0445\\u0446\\u0447\\u0448\\u0449\\u044a\\u044b\\u044c\\u044d\\u044e\\u044f\\u0451\\u0454\\u0456\\u0457\\u0491\\u2013!'(),-.:;? \",\n",
    "        punctuations=\"!'(),-.:;? \",\n",
    "        phonemes=\"\",\n",
    "        is_unique=True,\n",
    "        is_sorted=True,\n",
    "    ),\n",
    "    phoneme_cache_path=None,\n",
    "    precompute_num_workers=12,\n",
    "    start_by_longest=True,\n",
    "    datasets=DATASETS_CONFIG_LIST,\n",
    "    cudnn_benchmark=False,\n",
    "    max_audio_len=SAMPLE_RATE * MAX_AUDIO_LEN_IN_SECONDS,\n",
    "    mixed_precision=False,\n",
    "    test_sentences=[\n",
    "        [\n",
    "            \"‡∏â‡∏±‡∏ô‡πÉ‡∏ä‡πâ‡πÄ‡∏ß‡∏•‡∏≤‡∏ô‡∏≤‡∏ô‡∏°‡∏≤‡∏Å‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏û‡∏±‡∏í‡∏ô‡∏≤‡πÄ‡∏™‡∏µ‡∏¢‡∏á ‡πÅ‡∏•‡∏∞‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡∏â‡∏±‡∏ô‡∏°‡∏µ‡∏°‡∏±‡∏ô‡πÅ‡∏•‡πâ‡∏ß ‡∏â‡∏±‡∏ô‡∏à‡∏∞‡πÑ‡∏°‡πà‡πÄ‡∏á‡∏µ‡∏¢‡∏ö‡∏≠‡∏µ‡∏Å‡∏ï‡πà‡∏≠‡πÑ‡∏õ\",\n",
    "            \"VCTK_cv005\",\n",
    "            None,\n",
    "            \"th\",\n",
    "        ],\n",
    "        [\n",
    "            \"‡∏¢‡∏±‡∏Å‡∏©‡πå‡πÉ‡∏´‡∏ç‡πà‡πÑ‡∏•‡πà‡∏¢‡∏±‡∏Å‡∏©‡πå‡πÄ‡∏•‡πá‡∏Å ‡∏¢‡∏±‡∏Å‡∏©‡πå‡πÄ‡∏•‡πá‡∏Å‡πÑ‡∏•‡πà‡∏¢‡∏±‡∏Å‡∏©‡πå‡πÉ‡∏´‡∏ç‡πà\",\n",
    "            \"VCTK_cv068\",\n",
    "            None,\n",
    "            \"th\",\n",
    "        ],\n",
    "        [\n",
    "            \"‡∏¢‡∏≤‡∏¢‡∏Å‡∏¥‡∏ô‡∏•‡∏≥‡πÑ‡∏¢‡∏ô‡πâ‡∏≥‡∏•‡∏≤‡∏¢‡∏¢‡∏≤‡∏¢‡πÑ‡∏´‡∏•‡∏¢‡πâ‡∏≠‡∏¢\",\n",
    "            \"VCTK_cv057\",\n",
    "            None,\n",
    "            \"th\",\n",
    "        ],\n",
    "        [\n",
    "            \"‡∏ä‡∏≤‡∏°‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ß‡∏Ñ‡∏ß‡πà‡∏≥‡πÄ‡∏ä‡πâ‡∏≤ ‡∏ä‡∏≤‡∏°‡∏Ç‡∏≤‡∏°‡∏Ñ‡∏ß‡πà‡∏≥‡∏Ñ‡πà‡∏≥\",\n",
    "            \"VCTK_cv103\",\n",
    "            None,\n",
    "            \"th\",\n",
    "        ],\n",
    "        [\n",
    "            \"‡∏´‡∏°‡∏≠‡∏ô‡∏•‡∏≠‡∏¢‡∏ô‡πâ‡∏≥‡∏°‡∏≤‡∏ß‡πà‡∏≤‡∏¢‡∏ô‡πâ‡∏≥‡πÑ‡∏õ‡∏ñ‡∏≠‡∏¢‡∏´‡∏°‡∏≠‡∏ô\",\n",
    "            \"VCTK_cv133\",\n",
    "            None,\n",
    "            \"th\",\n",
    "        ],\n",
    "    ],\n",
    "    # Enable the weighted sampler\n",
    "    use_weighted_sampler=True,\n",
    "    # Ensures that all speakers are seen in the training batch equally no matter how many samples each speaker has\n",
    "    weighted_sampler_attrs={\"speaker_name\": 1.0},\n",
    "    weighted_sampler_multipliers={},\n",
    "    # It defines the Speaker Consistency Loss (SCL) Œ± to 9 like the paper\n",
    "    speaker_encoder_loss_alpha=9.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all the datasets samples and split traning and evaluation sets\n",
    "train_samples, eval_samples = load_tts_samples(\n",
    "    config.datasets,\n",
    "    eval_split=True,\n",
    "    eval_split_max_size=config.eval_split_max_size,\n",
    "    eval_split_size=config.eval_split_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init the model\n",
    "model = Vits.init_from_config(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init the trainer and üöÄ\n",
    "trainer = Trainer(\n",
    "    TrainerArgs(restore_path=RESTORE_PATH, skip_train_epoch=SKIP_TRAIN_EPOCH),\n",
    "    config,\n",
    "    output_path=OUT_PATH,\n",
    "    model=model,\n",
    "    train_samples=train_samples,\n",
    "    eval_samples=eval_samples,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capstone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
