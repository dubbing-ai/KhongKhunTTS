{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "from trainer import Trainer, TrainerArgs\n",
    "\n",
    "# from TTS.bin.compute_embeddings import compute_embeddings\n",
    "from compute_embeddings_wavLM import compute_embeddings # use custom formatter without forking the lib\n",
    "from TTS.bin.resample import resample_files\n",
    "from TTS.config.shared_configs import BaseDatasetConfig\n",
    "from TTS.tts.configs.vits_config import VitsConfig\n",
    "from TTS.tts.datasets import load_tts_samples\n",
    "from TTS.tts.models.vits import CharactersConfig, Vits, VitsArgs, VitsAudioConfig\n",
    "# from TTS.utils.downloaders import download_vctk\n",
    "# from TTS.config import load_config\n",
    "# from TTS.config.shared_configs import BaseDatasetConfig\n",
    "# from TTS.tts.datasets import load_tts_samples\n",
    "# from TTS.tts.utils.managers import save_file\n",
    "# from TTS.tts.utils.speakers import SpeakerManager\n",
    "from TTS.tts.datasets.formatters import vctk\n",
    "from functools import partial\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "torch.set_num_threads(24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Current path\n",
    "CURRENT_PATH = os.getcwd()\n",
    "\n",
    "# Name of the run for the Trainer\n",
    "RUN_NAME = \"KhongKhunTTS-Experiment02\"\n",
    "\n",
    "# Path where you want to save the models outputs (configs, checkpoints and tensorboard logs)\n",
    "OUT_PATH = os.path.join(CURRENT_PATH, \"runs\")\n",
    "\n",
    "# If you want to do transfer learning and speedup your training you can set here the path to the model\n",
    "RESTORE_PATH = \"./best_model_base.pth\"\n",
    "\n",
    "# This paramter is useful to debug, it skips the training epochs and just do the evaluation  and produce the test sentences\n",
    "SKIP_TRAIN_EPOCH = False\n",
    "\n",
    "# Set here the batch size to be used in training and evaluation\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Training Sampling rate and the target sampling rate for resampling the downloaded dataset (Note: If you change this you might need to redownload the dataset !!)\n",
    "# Note: If you add new datasets, please make sure that the dataset sampling rate and this parameter are matching, otherwise resample your audios\n",
    "SAMPLE_RATE = 16000\n",
    "\n",
    "# Max audio length in seconds to be used in training (every audio bigger than it will be ignored)\n",
    "MIN_AUDIO_LEN_IN_SECONDS = 1\n",
    "MAX_AUDIO_LEN_IN_SECONDS = 10\n",
    "\n",
    "# Define the number of threads used during the audio resampling\n",
    "NUM_RESAMPLE_THREADS = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init configs\n",
    "commonvoice_config = BaseDatasetConfig(\n",
    "    formatter=\"vctk_16k\",\n",
    "    dataset_name=\"commonvoice\",\n",
    "    meta_file_train=\"\",\n",
    "    meta_file_val=\"\",\n",
    "    path=os.path.join(CURRENT_PATH, \"commonvoice-to-vctk\"),\n",
    "    language=\"th\",\n",
    "    ignored_speakers=[\n",
    "        \"cv017\", # Female Teenager\n",
    "        \"cv048\", # Female Teenager\n",
    "        \"cv039\", # Female Adult\n",
    "        \"cv052\", # Female Adult\n",
    "        \"cv069\", # Male Teenager\n",
    "        \"cv054\", # Male Teenager\n",
    "        \"cv049\", # Male Adult\n",
    "        \"cv026\", # Male Adult\n",
    "    ], # For testing set\n",
    ")\n",
    "\n",
    "# vctk_config = BaseDatasetConfig(\n",
    "#     formatter=\"vctk_32k\",\n",
    "#     dataset_name=\"vctk\",\n",
    "#     meta_file_train=\"\",\n",
    "#     meta_file_val=\"\",\n",
    "#     path=os.path.join(CURRENT_PATH, \"commonvoice-to-vctk\"),\n",
    "#     language=\"th\",\n",
    "#     ignored_speakers=[\n",
    "#         \"cv017\", # Female Teenager\n",
    "#         \"cv048\", # Female Teenager\n",
    "#         \"cv039\", # Female Adult\n",
    "#         \"cv052\", # Female Adult\n",
    "#         \"cv069\", # Male Teenager\n",
    "#         \"cv054\", # Male Teenager\n",
    "#         \"cv049\", # Male Adult\n",
    "#         \"cv026\", # Male Adult\n",
    "#     ], # For testing set\n",
    "# )\n",
    "\n",
    "# Add here all datasets configs, in our case we just want to train with the VCTK dataset then we need to add just VCTK. Note: If you want to add new datasets, just add them here and it will automatically compute the speaker embeddings (d-vectors) for this new dataset :)\n",
    "DATASETS_CONFIG_LIST = [commonvoice_config]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract speaker embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPEAKER_ENCODER_CHECKPOINT_PATH = (\n",
    "    \"microsoft/wavlm-base-plus-sv\"\n",
    ")\n",
    "# SPEAKER_ENCODER_CONFIG_PATH = \"https://github.com/coqui-ai/TTS/releases/download/speaker_encoder_model/config_se.json\"\n",
    "\n",
    "D_VECTOR_FILES = []  # List of speaker embeddings/d-vectors to be used during the training\n",
    "\n",
    "vctk_16k = partial(\n",
    "    vctk,\n",
    "    wavs_path=\"wav16_silence_trimmed\",\n",
    ")\n",
    "\n",
    "# Iterates all the dataset configs checking if the speakers embeddings are already computated, if not compute it\n",
    "for dataset_conf in DATASETS_CONFIG_LIST:\n",
    "    # Check if the embeddings weren't already computed, if not compute it\n",
    "    embeddings_file = os.path.join(dataset_conf.path, \"dvector.pth\")\n",
    "    if not os.path.isfile(embeddings_file):\n",
    "        print(f\">>> Computing the speaker embeddings for the {dataset_conf.dataset_name} dataset\")\n",
    "        compute_embeddings(\n",
    "            SPEAKER_ENCODER_CHECKPOINT_PATH,\n",
    "            None,\n",
    "            embeddings_file,\n",
    "            formatter_name=dataset_conf.formatter,\n",
    "            formatter=vctk_16k if dataset_conf.formatter == \"vctk_16k\" else None,\n",
    "            dataset_name=dataset_conf.dataset_name,\n",
    "            dataset_path=dataset_conf.path,\n",
    "            meta_file_train=dataset_conf.meta_file_train,\n",
    "            meta_file_val=dataset_conf.meta_file_val,\n",
    "        )\n",
    "\n",
    "    D_VECTOR_FILES.append(embeddings_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audio config used in training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_config = VitsAudioConfig(\n",
    "    sample_rate=SAMPLE_RATE,\n",
    "    hop_length=256,\n",
    "    win_length=1024,\n",
    "    fft_size=1024,\n",
    "    mel_fmin=0.0,\n",
    "    mel_fmax=None,\n",
    "    num_mels=80,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init VITSArgs setting the arguments that are needed for the KhongKhunTTS model\n",
    "model_args = VitsArgs(\n",
    "    d_vector_file=D_VECTOR_FILES,\n",
    "    use_d_vector_file=True,\n",
    "    d_vector_dim=512,\n",
    "\n",
    "    num_layers_text_encoder=10,\n",
    "    resblock_type_decoder=\"2\",  # In the YourTTS paper, trained using ResNet blocks type 2, if you like you can use the ResNet blocks type 1 like the VITS model\n",
    "\n",
    "    speaker_encoder_model_path=None,\n",
    "    speaker_encoder_config_path=None,\n",
    "\n",
    "    # Fix stochastic duration predictor produce invalid values while inference\n",
    "    inference_noise_scale_dp=0.5,\n",
    "\n",
    "    # Useful parameters to enable the Speaker Consistency Loss (SCL) described in the paper\n",
    "    use_speaker_encoder_as_loss=True,\n",
    "    # Useful parameters to enable multilingual training\n",
    "    use_language_embedding=True,\n",
    "    embedded_language_dim=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General training config, here you can change the batch size and others useful parameters\n",
    "config = VitsConfig(\n",
    "    output_path=OUT_PATH,\n",
    "    model_args=model_args,\n",
    "    run_name=RUN_NAME,\n",
    "    project_name=\"KhongKhunTTS\",\n",
    "    run_description=\"\"\"\n",
    "            - KhongKhunTTS trained using CommonVoiceTH (VCTK structure) with transfer learning from TSync2\n",
    "        \"\"\",\n",
    "    dashboard_logger=\"tensorboard\",\n",
    "    logger_uri=None,\n",
    "    audio=audio_config,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    batch_group_size=48,\n",
    "    eval_batch_size=BATCH_SIZE,\n",
    "    num_loader_workers=8,\n",
    "    eval_split_max_size=256,\n",
    "    print_step=50,\n",
    "    plot_step=100,\n",
    "    log_model_step=1000,\n",
    "    save_step=5000,\n",
    "    save_n_checkpoints=2,\n",
    "    save_checkpoints=True,\n",
    "    target_loss=\"loss_1\",\n",
    "    print_eval=False,\n",
    "    use_phonemes=False,\n",
    "    phonemizer=\"espeak\",\n",
    "    phoneme_language=\"en\",\n",
    "    compute_input_seq_cache=True,\n",
    "    add_blank=True,\n",
    "    text_cleaner=\"multilingual_cleaners\",\n",
    "    characters=CharactersConfig(\n",
    "        characters_class=\"TTS.tts.models.vits.VitsCharacters\",\n",
    "        pad=\"_\",\n",
    "        eos=\"&\",\n",
    "        bos=\"*\",\n",
    "        blank=None,\n",
    "        characters=\"ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\\u00af\\u00b7\\u00df\\u00e0\\u00e1\\u00e2\\u00e3\\u00e4\\u00e6\\u00e7\\u00e8\\u00e9\\u00ea\\u00eb\\u00ec\\u00ed\\u00ee\\u00ef\\u00f1\\u00f2\\u00f3\\u00f4\\u00f5\\u00f6\\u00f9\\u00fa\\u00fb\\u00fc\\u00ff\\u0101\\u0105\\u0107\\u0113\\u0119\\u011b\\u012b\\u0131\\u0142\\u0144\\u014d\\u0151\\u0153\\u015b\\u016b\\u0171\\u017a\\u017c\\u01ce\\u01d0\\u01d2\\u01d4\\u0430\\u0431\\u0432\\u0433\\u0434\\u0435\\u0436\\u0437\\u0438\\u0439\\u043a\\u043b\\u043c\\u043d\\u043e\\u043f\\u0440\\u0441\\u0442\\u0443\\u0444\\u0445\\u0446\\u0447\\u0448\\u0449\\u044a\\u044b\\u044c\\u044d\\u044e\\u044f\\u0451\\u0454\\u0456\\u0457\\u0491\\u2013!\\\"'(),-.:;?|~ \\u0e01\\u0e02\\u0e04\\u0e06\\u0e07\\u0e08\\u0e09\\u0e0a\\u0e0b\\u0e0c\\u0e0d\\u0e0e\\u0e0f\\u0e10\\u0e11\\u0e12\\u0e13\\u0e14\\u0e15\\u0e16\\u0e17\\u0e18\\u0e19\\u0e1a\\u0e1b\\u0e1c\\u0e1d\\u0e1e\\u0e1f\\u0e20\\u0e21\\u0e22\\u0e23\\u0e24\\u0e25\\u0e27\\u0e28\\u0e29\\u0e2a\\u0e2b\\u0e2c\\u0e2d\\u0e2e\\u0e2f\\u0e30\\u0e31\\u0e32\\u0e33\\u0e34\\u0e35\\u0e36\\u0e37\\u0e38\\u0e39\\u0e40\\u0e41\\u0e42\\u0e43\\u0e44\\u0e45\\u0e46\\u0e47\\u0e48\\u0e49\\u0e4a\\u0e4b\\u0e4c\\u0e4d\\u2014\\u2018\\u2019\\u201c\\u201d\",\n",
    "        punctuations=\"!\\\"'(),-.:;?|~ \",\n",
    "        phonemes=\"\",\n",
    "        is_unique=True,\n",
    "        is_sorted=True,\n",
    "    ),\n",
    "    phoneme_cache_path=None,\n",
    "    precompute_num_workers=12,\n",
    "    start_by_longest=True,\n",
    "    datasets=DATASETS_CONFIG_LIST,\n",
    "    cudnn_benchmark=False,\n",
    "    min_audio_len=SAMPLE_RATE * MIN_AUDIO_LEN_IN_SECONDS,\n",
    "    max_audio_len=SAMPLE_RATE * MAX_AUDIO_LEN_IN_SECONDS,\n",
    "    mixed_precision=False,\n",
    "    test_sentences=[\n",
    "        [\n",
    "            \"‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡∏≠‡πà‡∏≤‡∏ô‡∏≠‡∏≠‡∏Å‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢\",\n",
    "            \"VCTK_cv005\",\n",
    "            None,\n",
    "            \"th\",\n",
    "        ],\n",
    "        [\n",
    "            \"‡∏¢‡∏±‡∏Å‡∏©‡πå‡πÉ‡∏´‡∏ç‡πà‡πÑ‡∏•‡πà‡∏¢‡∏±‡∏Å‡∏©‡πå‡πÄ‡∏•‡πá‡∏Å ‡∏¢‡∏±‡∏Å‡∏©‡πå‡πÄ‡∏•‡πá‡∏Å‡πÑ‡∏•‡πà‡∏¢‡∏±‡∏Å‡∏©‡πå‡πÉ‡∏´‡∏ç‡πà\",\n",
    "            \"VCTK_cv068\",\n",
    "            None,\n",
    "            \"th\",\n",
    "        ],\n",
    "        [\n",
    "            \"‡∏¢‡∏≤‡∏¢‡∏Å‡∏¥‡∏ô‡∏•‡∏≥‡πÑ‡∏¢ ‡∏ô‡πâ‡∏≥‡∏•‡∏≤‡∏¢‡∏¢‡∏≤‡∏¢‡πÑ‡∏´‡∏•‡∏¢‡πâ‡∏≠‡∏¢\",\n",
    "            \"VCTK_cv057\",\n",
    "            None,\n",
    "            \"th\",\n",
    "        ],\n",
    "        [\n",
    "            \"‡∏ä‡∏≤‡∏°‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ß‡∏Ñ‡∏ß‡πà‡∏≥‡πÄ‡∏ä‡πâ‡∏≤ ‡∏ä‡∏≤‡∏°‡∏Ç‡∏≤‡∏ß‡∏Ñ‡∏ß‡πà‡∏≥‡∏Ñ‡πà‡∏≥\",\n",
    "            \"VCTK_cv103\",\n",
    "            None,\n",
    "            \"th\",\n",
    "        ],\n",
    "        [\n",
    "            \"‡∏´‡∏°‡∏≠‡∏ô‡∏•‡∏≠‡∏¢‡∏ô‡πâ‡∏≥‡∏°‡∏≤ ‡∏ß‡πà‡∏≤‡∏¢‡∏ô‡πâ‡∏≥‡πÑ‡∏õ ‡∏ñ‡∏≠‡∏¢‡∏´‡∏°‡∏≠‡∏ô\",\n",
    "            \"VCTK_cv133\",\n",
    "            None,\n",
    "            \"th\",\n",
    "        ],\n",
    "        [\n",
    "            \"‡πÄ‡∏ä‡πâ‡∏≤‡∏ü‡∏≤‡∏î‡∏ú‡∏±‡∏î‡∏ü‡∏±‡∏Å ‡πÄ‡∏¢‡πá‡∏ô‡∏ü‡∏≤‡∏î‡∏ü‡∏±‡∏Å‡∏ú‡∏±‡∏î\",\n",
    "            \"VCTK_cv128\",\n",
    "            None,\n",
    "            \"th\",\n",
    "        ],\n",
    "    ],\n",
    "    # # Enable the weighted sampler\n",
    "    use_weighted_sampler=True,\n",
    "    # # Ensures that all speakers are seen in the training batch equally no matter how many samples each speaker has\n",
    "    weighted_sampler_attrs={\"speaker_name\": 1.0},\n",
    "    # weighted_sampler_multipliers={},\n",
    "    weighted_sampler_multipliers={\"temp\": None},\n",
    "\n",
    "    # It defines the Speaker Consistency Loss (SCL) Œ± to 9 like the paper\n",
    "    speaker_encoder_loss_alpha=9.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all the datasets samples and split traning and evaluation sets\n",
    "train_samples, eval_samples = load_tts_samples(\n",
    "    config.datasets,\n",
    "    eval_split=True,\n",
    "    eval_split_max_size=config.eval_split_max_size,\n",
    "    eval_split_size=config.eval_split_size,\n",
    "    formatter=vctk_16k if commonvoice_config.formatter == \"vctk_16k\" else None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init the model\n",
    "from CustomVits import CustomVits\n",
    "\n",
    "model = CustomVits.init_from_config(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init the trainer and üöÄ\n",
    "trainer = Trainer(\n",
    "    TrainerArgs(restore_path=RESTORE_PATH, skip_train_epoch=SKIP_TRAIN_EPOCH),\n",
    "    config,\n",
    "    output_path=OUT_PATH,\n",
    "    model=model,\n",
    "    train_samples=train_samples,\n",
    "    eval_samples=eval_samples,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_checkpoint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "khongkhuntts-CfIBPH6q-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
