{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import IPython\n",
    "from IPython.display import Audio\n",
    "import torch\n",
    "import os\n",
    "\n",
    "from TTS.tts.utils.synthesis import synthesis\n",
    "try:\n",
    "  from TTS.utils.audio import AudioProcessor\n",
    "except:\n",
    "  from TTS.utils.audio import AudioProcessor\n",
    "from TTS.tts.models import setup_model\n",
    "from TTS.config import load_config\n",
    "from TTS.tts.models.vits import *\n",
    "from TTS.tts.utils.speakers import SpeakerManager\n",
    "\n",
    "\n",
    "from TTS.bin.resample import resample_files\n",
    "from TTS.utils.vad import get_vad_model_and_utils, remove_silence\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Parameter and Constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_MODEL_PATH = '../model'\n",
    "\n",
    "# model vars \n",
    "MODEL_PATH = os.path.join(BASE_MODEL_PATH, 'best_model_160217.pth')\n",
    "CONFIG_PATH = os.path.join(BASE_MODEL_PATH, 'config.json')\n",
    "TTS_LANGUAGES = os.path.join(BASE_MODEL_PATH, 'language_ids.json')\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "\n",
    "REFERENCE_PATH = './dataset/reference_wav/'\n",
    "\n",
    "model_name = MODEL_PATH.split(\".\")[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Model and Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > Setting up Audio Processor...\n",
      " | > sample_rate:16000\n",
      " | > resample:False\n",
      " | > num_mels:80\n",
      " | > log_func:np.log10\n",
      " | > min_level_db:0\n",
      " | > frame_shift_ms:None\n",
      " | > frame_length_ms:None\n",
      " | > ref_level_db:None\n",
      " | > fft_size:1024\n",
      " | > power:None\n",
      " | > preemphasis:0.0\n",
      " | > griffin_lim_iters:None\n",
      " | > signal_norm:None\n",
      " | > symmetric_norm:None\n",
      " | > mel_fmin:0\n",
      " | > mel_fmax:None\n",
      " | > pitch_fmin:None\n",
      " | > pitch_fmax:None\n",
      " | > spec_gain:20.0\n",
      " | > stft_pad_mode:reflect\n",
      " | > max_norm:1.0\n",
      " | > clip_norm:True\n",
      " | > do_trim_silence:False\n",
      " | > trim_db:60\n",
      " | > do_sound_norm:False\n",
      " | > do_amp_to_db_linear:True\n",
      " | > do_amp_to_db_mel:True\n",
      " | > do_rms_norm:False\n",
      " | > db_level:None\n",
      " | > stats_path:None\n",
      " | > base:10\n",
      " | > hop_length:256\n",
      " | > win_length:1024\n",
      " > Using model: vits\n",
      " > Setting up Audio Processor...\n",
      " | > sample_rate:16000\n",
      " | > resample:False\n",
      " | > num_mels:80\n",
      " | > log_func:np.log10\n",
      " | > min_level_db:0\n",
      " | > frame_shift_ms:None\n",
      " | > frame_length_ms:None\n",
      " | > ref_level_db:None\n",
      " | > fft_size:1024\n",
      " | > power:None\n",
      " | > preemphasis:0.0\n",
      " | > griffin_lim_iters:None\n",
      " | > signal_norm:None\n",
      " | > symmetric_norm:None\n",
      " | > mel_fmin:0\n",
      " | > mel_fmax:None\n",
      " | > pitch_fmin:None\n",
      " | > pitch_fmax:None\n",
      " | > spec_gain:20.0\n",
      " | > stft_pad_mode:reflect\n",
      " | > max_norm:1.0\n",
      " | > clip_norm:True\n",
      " | > do_trim_silence:False\n",
      " | > trim_db:60\n",
      " | > do_sound_norm:False\n",
      " | > do_amp_to_db_linear:True\n",
      " | > do_amp_to_db_mel:True\n",
      " | > do_rms_norm:False\n",
      " | > db_level:None\n",
      " | > stats_path:None\n",
      " | > base:10\n",
      " | > hop_length:256\n",
      " | > win_length:1024\n",
      " > Model fully restored. \n",
      " > Setting up Audio Processor...\n",
      " | > sample_rate:16000\n",
      " | > resample:False\n",
      " | > num_mels:64\n",
      " | > log_func:np.log10\n",
      " | > min_level_db:-100\n",
      " | > frame_shift_ms:None\n",
      " | > frame_length_ms:None\n",
      " | > ref_level_db:20\n",
      " | > fft_size:512\n",
      " | > power:1.5\n",
      " | > preemphasis:0.97\n",
      " | > griffin_lim_iters:60\n",
      " | > signal_norm:False\n",
      " | > symmetric_norm:False\n",
      " | > mel_fmin:0\n",
      " | > mel_fmax:8000.0\n",
      " | > pitch_fmin:1.0\n",
      " | > pitch_fmax:640.0\n",
      " | > spec_gain:20.0\n",
      " | > stft_pad_mode:reflect\n",
      " | > max_norm:4.0\n",
      " | > clip_norm:False\n",
      " | > do_trim_silence:False\n",
      " | > trim_db:60\n",
      " | > do_sound_norm:False\n",
      " | > do_amp_to_db_linear:True\n",
      " | > do_amp_to_db_mel:True\n",
      " | > do_rms_norm:True\n",
      " | > db_level:-27.0\n",
      " | > stats_path:None\n",
      " | > base:10\n",
      " | > hop_length:160\n",
      " | > win_length:400\n",
      " > initialization of language-embedding layers.\n"
     ]
    }
   ],
   "source": [
    "# load the config\n",
    "C = load_config(CONFIG_PATH)\n",
    "\n",
    "# load the audio processor\n",
    "ap = AudioProcessor(**C.audio)\n",
    "\n",
    "# override config\n",
    "C[\"speakers_file\"] = None\n",
    "C[\"d_vector_file\"] = []\n",
    "C[\"language_ids_file\"] = TTS_LANGUAGES\n",
    "\n",
    "C[\"model_args\"][\"speakers_file\"] = None\n",
    "C[\"model_args\"][\"d_vector_file\"] = []\n",
    "C[\"model_args\"][\"language_ids_file\"] = TTS_LANGUAGES\n",
    "\n",
    "C.model_args['use_speaker_encoder_as_loss'] = False\n",
    "\n",
    "model = setup_model(C)\n",
    "cp = torch.load(MODEL_PATH, map_location=torch.device('cpu'))\n",
    "\n",
    "# remove speaker encoder\n",
    "model_weights = cp['model'].copy()\n",
    "for key in list(model_weights.keys()):\n",
    "  if \"speaker_encoder\" in key:\n",
    "    del model_weights[key]\n",
    "\n",
    "model.load_state_dict(model_weights)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "if USE_CUDA:\n",
    "  model = model.cuda()\n",
    "\n",
    "# synthesize voice\n",
    "use_griffin_lim = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language ID: 0, Language Name: th\n"
     ]
    }
   ],
   "source": [
    "# Select language\n",
    "language_id = 0\n",
    "language_name_to_id = model.language_manager.name_to_id\n",
    "language_id_to_name = {v: k for k, v in language_name_to_id.items()}\n",
    "print(f\"Language ID: {language_id}, Language Name: {language_id_to_name[language_id]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup duration predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.length_scale = 1  # scaler for the duration predictor. The larger it is, the slower the speech.\n",
    "model.inference_noise_scale = 0.2 # defines the noise variance applied to the random z vector at inference.\n",
    "model.inference_noise_scale_dp = 0.2 # defines the noise variance applied to the duration predictor z vector at inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process reference audio file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resampling the audio files...\n",
      "Found 28 files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:04<00:00,  5.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done !\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# reamples the audio file to match the sample rate of the model\n",
    "resample_files(REFERENCE_PATH, C.audio['sample_rate'], file_ext=\"wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/snakers4/silero-vad/zipball/master\" to C:\\Users\\Ming/.cache\\torch\\hub\\master.zip\n",
      "100%|██████████| 28/28 [00:05<00:00,  4.68it/s]\n"
     ]
    }
   ],
   "source": [
    "# trim silence at the beginning and end of the audio\n",
    "model_and_utils = get_vad_model_and_utils(use_cuda=USE_CUDA, use_onnx=False)\n",
    "\n",
    "for file in tqdm(os.listdir(REFERENCE_PATH)):\n",
    "  if not file.endswith(\".wav\"):\n",
    "    continue\n",
    "  output_path, is_speech = remove_silence(\n",
    "    model_and_utils,\n",
    "    os.path.join(REFERENCE_PATH, file),\n",
    "    os.path.join(REFERENCE_PATH, file),\n",
    "    trim_just_beginning_and_end=True,\n",
    "    use_cuda=USE_CUDA\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in os.listdir(REFERENCE_PATH):\n",
    "    if not file.endswith(\".wav\"):\n",
    "        continue\n",
    "    REFERENCE_FILE = os.path.join(REFERENCE_PATH, file)\n",
    "    # normalize the reference audio with rms to -27dB\n",
    "    !ffmpeg-normalize $REFERENCE_FILE -nt rms -t=-27 -o $REFERENCE_FILE -ar 16000 -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > Model fully restored. \n",
      " > Setting up Audio Processor...\n",
      " | > sample_rate:16000\n",
      " | > resample:False\n",
      " | > num_mels:64\n",
      " | > log_func:np.log10\n",
      " | > min_level_db:-100\n",
      " | > frame_shift_ms:None\n",
      " | > frame_length_ms:None\n",
      " | > ref_level_db:20\n",
      " | > fft_size:512\n",
      " | > power:1.5\n",
      " | > preemphasis:0.97\n",
      " | > griffin_lim_iters:60\n",
      " | > signal_norm:False\n",
      " | > symmetric_norm:False\n",
      " | > mel_fmin:0\n",
      " | > mel_fmax:8000.0\n",
      " | > pitch_fmin:1.0\n",
      " | > pitch_fmax:640.0\n",
      " | > spec_gain:20.0\n",
      " | > stft_pad_mode:reflect\n",
      " | > max_norm:4.0\n",
      " | > clip_norm:False\n",
      " | > do_trim_silence:False\n",
      " | > trim_db:60\n",
      " | > do_sound_norm:False\n",
      " | > do_amp_to_db_linear:True\n",
      " | > do_amp_to_db_mel:True\n",
      " | > do_rms_norm:True\n",
      " | > db_level:-27.0\n",
      " | > stats_path:None\n",
      " | > base:10\n",
      " | > hop_length:160\n",
      " | > win_length:400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:04<00:00,  6.38it/s]\n"
     ]
    }
   ],
   "source": [
    "SE_speaker_manager = SpeakerManager(encoder_model_path=C[\"model_args\"][\"speaker_encoder_model_path\"], encoder_config_path=C[\"model_args\"][\"speaker_encoder_config_path\"], use_cuda=USE_CUDA)\n",
    "\n",
    "for file in tqdm(os.listdir(REFERENCE_PATH)):\n",
    "    if not file.endswith(\".wav\"):\n",
    "        continue\n",
    "    filename = file.split(\".\")[0]\n",
    "    REFERENCE_FILE = os.path.join(REFERENCE_PATH, file)\n",
    "    reference_emb = SE_speaker_manager.compute_embedding_from_clip(REFERENCE_FILE)\n",
    "    with open(f\"dataset/transcripts/{filename}.txt\", \"r\") as f:\n",
    "        text = \"\".join(f.readline().split(\"|\")).strip()\n",
    "    wav, alignment, _, _ = synthesis(\n",
    "                    model = model,\n",
    "                    text = text,\n",
    "                    CONFIG = C,\n",
    "                    use_cuda = USE_CUDA,\n",
    "                    d_vector = reference_emb,\n",
    "                    style_wav = None,\n",
    "                    language_id = language_id,\n",
    "                    use_griffin_lim = True,\n",
    "                    do_trim_silence = False,\n",
    "                ).values()\n",
    "    ap.save_wav(wav, f\"dataset/synthesized_wav/{filename}.wav\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare Reference Vs Synthesized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference Lengths: [3.194, 4.562, 7.378, 2.772, 9.33, 4.372, 2.454, 2.93, 5.81, 3.482, 5.59, 5.074, 4.764, 6.962, 7.634, 3.604, 3.922, 4.38, 6.78, 6.938, 3.986, 1.844, 3.026, 5.332, 5.366, 5.752, 8.466, 2.68]\n",
      "Synthesized Lengths: [2.56, 4.128, 4.864, 2.336, 8.496, 3.792, 2.112, 2.336, 4.128, 2.928, 4.432, 4.016, 3.968, 5.952, 6.016, 2.864, 3.232, 3.392, 5.616, 5.776, 3.376, 1.552, 2.544, 4.272, 4.656, 5.024, 7.104, 2.192]\n"
     ]
    }
   ],
   "source": [
    "def get_audio_lengths(audio_folder):\n",
    "    audio_lengths = []\n",
    "    for file in os.listdir(audio_folder):\n",
    "        if not file.endswith(\".wav\"):\n",
    "            continue\n",
    "        audio_lengths.append(ap.get_duration(os.path.join(audio_folder, file)))\n",
    "    return audio_lengths\n",
    "\n",
    "reference_lengths = get_audio_lengths(REFERENCE_PATH)\n",
    "synthesized_lengths = get_audio_lengths(\"dataset/synthesized_wav\")\n",
    "\n",
    "print(f\"Reference Lengths: {reference_lengths}\")\n",
    "print(f\"Synthesized Lengths: {synthesized_lengths}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ratios: [0.80150282 0.90486629 0.65925725 0.84271284 0.91061093 0.8673376\n",
      " 0.8606357  0.79726962 0.71049914 0.84089604 0.79284436 0.79148601\n",
      " 0.83291352 0.85492675 0.78805345 0.79467259 0.82406935 0.77442922\n",
      " 0.82831858 0.83251658 0.84696438 0.84164859 0.84071381 0.8012003\n",
      " 0.86768543 0.87343533 0.83912119 0.81791045]\n",
      "Harmonic Mean: 0.8193357884680831\n",
      "length_scale: 1.220500817948775\n"
     ]
    }
   ],
   "source": [
    "reference_lengths = np.array(reference_lengths)\n",
    "synthesized_lengths = np.array(synthesized_lengths)\n",
    "\n",
    "ratios = synthesized_lengths / reference_lengths\n",
    "print(f\"Ratios: {ratios}\")\n",
    "harmonic_mean = len(ratios) / np.sum(1 / ratios)\n",
    "print(f\"Harmonic Mean: {harmonic_mean}\")\n",
    "print(f\"length_scale: {1/harmonic_mean}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "voicecraft",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
